{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e67269-2a81-43a9-8bf9-2f7ac4dfae19",
   "metadata": {},
   "source": [
    "# Ejemplo de problema típico de clasificación de textos\n",
    "\n",
    "En este ejemplo se elige un dataset de clasificación de texto y se explican los principales pasos que se suelen llevar a cabo a la hora de obtener los mejores resultados, desde la obtención de un baseline rápido a un modelo del lenguaje finetuneado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a947882-a481-4180-a5e8-9dd589a498a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datos\n",
    "\n",
    "[Coronavirus tweets NLP - Text Classification](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification]) es un dataset que consta de 40k tweets recogidos durante marzo de 2020 y etiquetados manualmente según el sentimiento expresado (relacionados con la pandemia, en general): `Extremadamente Negativo`, `Negativo`, `Neutro`, `Positivo`, `Extremadamente Positivo`.\n",
    "\n",
    "Este tipo de tareas de clasificación de tweets son muy comunes en la industria ya que a las empresas les gusta saber el impacto de sus campañas o la imagen que los usuarios tienen de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c719ea-20c0-49b1-8638-9201d4d39544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>44952</td>\n",
       "      <td>89904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>44953</td>\n",
       "      <td>89905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>You know itÂs getting tough when @KameronWild...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>44954</td>\n",
       "      <td>89906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location     TweetAt  \\\n",
       "0          3799       48751                        London  16-03-2020   \n",
       "1          3800       48752                            UK  16-03-2020   \n",
       "2          3801       48753                     Vagabonds  16-03-2020   \n",
       "3          3802       48754                           NaN  16-03-2020   \n",
       "4          3803       48755                           NaN  16-03-2020   \n",
       "...         ...         ...                           ...         ...   \n",
       "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
       "41153     44952       89904                           NaN  14-04-2020   \n",
       "41154     44953       89905                           NaN  14-04-2020   \n",
       "41155     44954       89906                           NaN  14-04-2020   \n",
       "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1      advice Talk to your neighbours family to excha...            Positive  \n",
       "2      Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3      My food stock is not the only one which is emp...            Positive  \n",
       "4      Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
       "...                                                  ...                 ...  \n",
       "41152  Airline pilots offering to stock supermarket s...             Neutral  \n",
       "41153  Response to complaint not provided citing COVI...  Extremely Negative  \n",
       "41154  You know itÂs getting tough when @KameronWild...            Positive  \n",
       "41155  Is it wrong that the smell of hand sanitizer i...             Neutral  \n",
       "41156  @TartiiCat Well new/used Rift S are going for ...            Negative  \n",
       "\n",
       "[41157 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"datos/Corona_NLP_train.csv\", encoding=\"latin-1\")\n",
    "test = pd.read_csv(\"datos/Corona_NLP_test.csv\", encoding=\"latin-1\")\n",
    "cuantos = 5000\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ba3eb3-d7c0-4b16-9b0b-b3c7814d1ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"Sentiment\"][:cuantos].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45995a32-bee1-4915-af5f-c05688109bc0",
   "metadata": {},
   "source": [
    "Las etiquetas están relativamente equilibradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2dde59d-435e-4700-b12f-8ccebbecb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"Extremely Negative\": 0,\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 2,\n",
    "    \"Positive\": 3,\n",
    "    \"Extremely Positive\": 4,\n",
    "}\n",
    "\n",
    "X_train = train[\"OriginalTweet\"][:cuantos]\n",
    "y_train = train[\"Sentiment\"].apply(lambda x: label2id[x])[:cuantos]\n",
    "X_test = test[\"OriginalTweet\"][:cuantos]\n",
    "y_test = test[\"Sentiment\"].apply(lambda x: label2id[x])[:cuantos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37daf79d-2ea9-4750-89db-587cf65c4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd0b6d-6a5e-4ecd-9473-8a884210ffdf",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b86d4d-6062-494f-945a-da6654fefbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170eac47-f4e1-4df4-a6fe-7748a2144733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.605     0.166     0.260       592\n",
      "          Negative      0.384     0.514     0.440      1041\n",
      "           Neutral      0.465     0.543     0.501       619\n",
      "          Positive      0.341     0.507     0.407       947\n",
      "Extremely Positive      0.667     0.124     0.208       599\n",
      "\n",
      "          accuracy                          0.401      3798\n",
      "         macro avg      0.492     0.371     0.363      3798\n",
      "      weighted avg      0.465     0.401     0.377      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, list(y_train))\n",
    "print(classification_report(list(y_test), pipeline.predict(X_test), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f3b4f-adba-4b7f-9340-48f8b10e5749",
   "metadata": {},
   "source": [
    "#### Búsqueda de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c884f9-b2af-4c1b-badc-433900af2329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer__ngram_range': (5, 7), 'vectorizer__analyzer': 'char_wb', 'classifier__n_estimators': 274, 'classifier__min_samples_split': 3, 'classifier__min_samples_leaf': 2, 'classifier__max_features': 0.24999999999999992, 'classifier__criterion': 'gini'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.532     0.279     0.366       592\n",
      "          Negative      0.415     0.456     0.435      1041\n",
      "           Neutral      0.468     0.628     0.536       619\n",
      "          Positive      0.366     0.442     0.401       947\n",
      "Extremely Positive      0.560     0.344     0.426       599\n",
      "\n",
      "          accuracy                          0.435      3798\n",
      "         macro avg      0.468     0.430     0.433      3798\n",
      "      weighted avg      0.453     0.435     0.431      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", CountVectorizer()),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1,3), (3, 3), (3,5), (5, 7)],\n",
    "    \"vectorizer__analyzer\": [\"word\", \"char\", \"char_wb\"],\n",
    "    \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"classifier__max_features\": np.arange(0.1, 1, 0.01),\n",
    "    \"classifier__min_samples_leaf\": np.arange(1, 11),\n",
    "    \"classifier__min_samples_split\": np.arange(1, 17),\n",
    "    \"classifier__n_estimators\": np.arange(10, 500),\n",
    "}\n",
    "# La f1 macro tiene en cuenta todas las clases por igual al hacer la media\n",
    "search = RandomizedSearchCV(pipeline, params, scoring=\"f1_macro\", cv=3, n_jobs=-1, n_iter=10)\n",
    "search.fit(X_train, list(y_train))\n",
    "print(search.best_params_)\n",
    "print(classification_report(list(y_test), search.best_estimator_.predict(X_test), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8addb-d43a-4558-a07b-dc93aa588f4b",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e1e72d-d4b4-4846-adda-39defd164b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c594d39-3e92-40f4-8fcc-6501339f9fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.535     0.193     0.283       592\n",
      "          Negative      0.354     0.457     0.399      1041\n",
      "           Neutral      0.435     0.494     0.463       619\n",
      "          Positive      0.338     0.501     0.403       947\n",
      "Extremely Positive      0.722     0.160     0.262       599\n",
      "\n",
      "          accuracy                          0.386      3798\n",
      "         macro avg      0.477     0.361     0.362      3798\n",
      "      weighted avg      0.449     0.386     0.371      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer()),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, list(y_train))\n",
    "print(classification_report(list(y_test), pipeline.predict(X_test), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8016b47-4860-4d82-b942-628d6476285d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 42.66s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 44.27s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 40.40s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 83.01s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 84.15s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 84.70s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 86.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 87.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 96.75s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 99.63s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 98.22s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 102.51s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 102.97s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 103.09s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/home/guillem.garcia/.conda/envs/charla/lib/python3.9/site-packages/sklearn/pipeline.py:348: UserWarning: Persisting input arguments took 110.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer__ngram_range': (5, 7), 'vectorizer__analyzer': 'char_wb', 'classifier__n_estimators': 150, 'classifier__min_samples_split': 12, 'classifier__min_samples_leaf': 4, 'classifier__max_features': 0.5999999999999998, 'classifier__criterion': 'entropy'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.455     0.294     0.357       592\n",
      "          Negative      0.401     0.390     0.396      1041\n",
      "           Neutral      0.404     0.611     0.486       619\n",
      "          Positive      0.346     0.393     0.368       947\n",
      "Extremely Positive      0.522     0.342     0.413       599\n",
      "\n",
      "          accuracy                          0.404      3798\n",
      "         macro avg      0.426     0.406     0.404      3798\n",
      "      weighted avg      0.415     0.404     0.400      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer()),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1,3), (3, 3), (3,5), (5, 7)],\n",
    "    \"vectorizer__analyzer\": [\"word\", \"char\", \"char_wb\"],\n",
    "    \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"classifier__max_features\": np.arange(0.1, 1, 0.01),\n",
    "    \"classifier__min_samples_leaf\": np.arange(1, 11),\n",
    "    \"classifier__min_samples_split\": np.arange(1, 17),\n",
    "    \"classifier__n_estimators\": np.arange(10, 500),\n",
    "}\n",
    "# La f1 macro tiene en cuenta todas las clases por igual al hacer la media\n",
    "search = RandomizedSearchCV(pipeline, params, scoring=\"f1_macro\", cv=3, n_jobs=-1, n_iter=10)\n",
    "search.fit(X_train, list(y_train))\n",
    "print(search.best_params_)\n",
    "print(classification_report(list(y_test), search.best_estimator_.predict(X_test), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7639b3bd-9e38-4eb3-a9a5-09504150b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer__ngram_range': (5, 7), 'vectorizer__analyzer': 'char_wb', 'classifier__n_estimators': 150, 'classifier__min_samples_split': 12, 'classifier__min_samples_leaf': 4, 'classifier__max_features': 0.5999999999999998, 'classifier__criterion': 'entropy'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.455     0.294     0.357       592\n",
      "          Negative      0.401     0.390     0.396      1041\n",
      "           Neutral      0.404     0.611     0.486       619\n",
      "          Positive      0.346     0.393     0.368       947\n",
      "Extremely Positive      0.522     0.342     0.413       599\n",
      "\n",
      "          accuracy                          0.404      3798\n",
      "         macro avg      0.426     0.406     0.404      3798\n",
      "      weighted avg      0.415     0.404     0.400      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(search.best_params_)\n",
    "print(classification_report(list(y_test), search.best_estimator_.predict(X_test), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810bc8c-70bf-4376-b7e3-2ed651ee4d2c",
   "metadata": {},
   "source": [
    "## SpaCy (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988abe91-2d97-4101-94fc-a558abe82639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def vectorize(X):\n",
    "    return [x.vector for x in nlp.pipe(X)]\n",
    "\n",
    "X_train_vs = vectorize(X_train)\n",
    "X_test_vs = vectorize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b4a57a-1657-42a8-b9c5-dd0e4d1bc0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.477     0.284     0.356       592\n",
      "          Negative      0.348     0.458     0.396      1041\n",
      "           Neutral      0.456     0.338     0.388       619\n",
      "          Positive      0.300     0.460     0.364       947\n",
      "Extremely Positive      0.536     0.150     0.235       599\n",
      "\n",
      "          accuracy                          0.363      3798\n",
      "         macro avg      0.424     0.338     0.348      3798\n",
      "      weighted avg      0.404     0.363     0.355      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(X_train_vs, list(y_train))\n",
    "print(classification_report(list(y_test), model.predict(X_test_vs), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "605a1992-710a-44c7-b6cd-b2512e680d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 90, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 0.7499999999999997, 'criterion': 'entropy'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.481     0.272     0.347       592\n",
      "          Negative      0.369     0.459     0.409      1041\n",
      "           Neutral      0.466     0.344     0.396       619\n",
      "          Positive      0.311     0.477     0.376       947\n",
      "Extremely Positive      0.512     0.219     0.306       599\n",
      "\n",
      "          accuracy                          0.378      3798\n",
      "         macro avg      0.428     0.354     0.367      3798\n",
      "      weighted avg      0.410     0.378     0.373      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "params = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_features\": np.arange(0.1, 1, 0.01),\n",
    "    \"min_samples_leaf\": np.arange(1, 11),\n",
    "    \"min_samples_split\": np.arange(1, 17),\n",
    "    \"n_estimators\": np.arange(10, 500),\n",
    "}\n",
    "# La f1 macro tiene en cuenta todas las clases por igual al hacer la media\n",
    "search = RandomizedSearchCV(model, params, scoring=\"f1_macro\", cv=3, n_jobs=-1, n_iter=10)\n",
    "search.fit(X_train_vs, list(y_train))\n",
    "print(search.best_params_)\n",
    "print(classification_report(list(y_test), search.best_estimator_.predict(X_test_vs), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43107da-d6e2-4805-a0ac-f06c1e6cc5af",
   "metadata": {},
   "source": [
    "## Sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19778c2d-b77f-4f8f-a070-c231404573cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "vectorizer = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "X_train_vt = vectorizer.encode(X_train)\n",
    "X_test_vt = vectorizer.encode(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a76fe01-26fa-4f03-972f-6329c1454da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.434     0.201     0.275       592\n",
      "          Negative      0.342     0.505     0.408      1041\n",
      "           Neutral      0.417     0.231     0.297       619\n",
      "          Positive      0.305     0.507     0.380       947\n",
      "Extremely Positive      0.500     0.057     0.102       599\n",
      "\n",
      "          accuracy                          0.343      3798\n",
      "         macro avg      0.400     0.300     0.293      3798\n",
      "      weighted avg      0.384     0.343     0.314      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(X_train_vt, list(y_train))\n",
    "print(classification_report(list(y_test), model.predict(X_test_vt), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11702f1c-4792-43bd-b23e-162715322697",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_features': array([0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ,\n",
       "       0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31,\n",
       "       0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42,\n",
       "       0.43, 0.44, 0.45, 0.46, 0...\n",
       "       413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
       "       426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
       "       439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "       452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464,\n",
       "       465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477,\n",
       "       478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
       "       491, 492, 493, 494, 495, 496, 497, 498, 499])},\n",
       "                   scoring='f1_macro')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "params = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_features\": np.arange(0.1, 1, 0.01),\n",
    "    \"min_samples_leaf\": np.arange(1, 11),\n",
    "    \"min_samples_split\": np.arange(1, 17),\n",
    "    \"n_estimators\": np.arange(10, 500),\n",
    "}\n",
    "# La f1 macro tiene en cuenta todas las clases por igual al hacer la media\n",
    "search = RandomizedSearchCV(model, params, scoring=\"f1_macro\", cv=3, n_jobs=-1, n_iter=10)\n",
    "search.fit(X_train_vt, list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c8992d3-01fb-4d4e-98f8-c3eddc067bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 294, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.4999999999999998, 'criterion': 'entropy'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.473     0.264     0.338       592\n",
      "          Negative      0.353     0.487     0.409      1041\n",
      "           Neutral      0.476     0.288     0.359       619\n",
      "          Positive      0.324     0.545     0.406       947\n",
      "Extremely Positive      0.613     0.063     0.115       599\n",
      "\n",
      "          accuracy                          0.367      3798\n",
      "         macro avg      0.448     0.329     0.325      3798\n",
      "      weighted avg      0.425     0.367     0.343      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(search.best_params_)\n",
    "print(classification_report(list(y_test), search.best_estimator_.predict(X_test_vt), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d287c-d8a9-41f6-9a04-29ddf2656646",
   "metadata": {},
   "source": [
    "A parte del Random Forest, las SVM lineares suelen dar resultados bastante buenos en problemas donde la carga gramatical es menor y, por lo tanto, son fáciles de resolver con conteo de palabras y de forma lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50c4c3-91e1-47cd-aca8-1923442467ab",
   "metadata": {},
   "source": [
    "## Fine Tuning DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68e865-2e44-49b7-a5c6-ba8225781a20",
   "metadata": {},
   "source": [
    "**Imprescindible** tener una máquina con GPU, si no no vale la pena ni probar. Google colab es una buena opción gratuita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a761e8d-7362-4f22-81be-278d3b90f19b",
   "metadata": {},
   "source": [
    "Primero cargamos el modelo y el tokenizador, asignamos los parámetros que queremos que tenga la red y elegimos la métrica que vamos a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "707a940d-7edc-4d0c-80a5-bf39816d99c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /home/guillem.garcia/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /home/guillem.garcia/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /home/guillem.garcia/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /home/guillem.garcia/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /home/guillem.garcia/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /home/guillem.garcia/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sentiment-analysis\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Extremely Negative\",\n",
      "    \"1\": \"Negative\",\n",
      "    \"2\": \"Neutral\",\n",
      "    \"3\": \"Positive\",\n",
      "    \"4\": \"Extremely Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"Extremely Negative\": 0,\n",
      "    \"Extremely Positive\": 4,\n",
      "    \"Negative\": 1,\n",
      "    \"Neutral\": 2,\n",
      "    \"Positive\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "model_name = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, model_max_length=512)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    hidden_dropout_prob=0.1, #\n",
    "    label2id=label2id,\n",
    "    id2label={i: t for t, i in label2id.items()},\n",
    "    finetuning_task=\"sentiment-analysis\",\n",
    "    problem_type=\"single_label_classification\",\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"distil_prueba\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    ")\n",
    "\n",
    "metric = datasets.load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21a53c-1bf2-41da-acc6-35ef168ad41d",
   "metadata": {},
   "source": [
    "Formateamos los datos para que estén en formato Dataset (también se podría usar un dataloader de pytorch o su equivalente de tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "528607b4-fde5-4176-87d0-9ab136ad1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd98654138b643d5b927df0d97cfe9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9db536570b414a8a54321cd0641769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_n, X_val, y_train_n, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "data = datasets.DatasetDict({\n",
    "    \"train\": datasets.Dataset.from_dict({\"text\": X_train_n, \"labels\": y_train_n}),\n",
    "    \"validation\": datasets.Dataset.from_dict({\"text\": X_val, \"labels\": y_val}),\n",
    "})\n",
    "\n",
    "\n",
    "def tokenize_and_create_labels(samples):\n",
    "    tokenized = tokenizer(samples[\"text\"], truncation=True)\n",
    "    tokenized[\"labels\"] = samples[\"labels\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_dataset = data.map(\n",
    "    tokenize_and_create_labels,\n",
    "    batched=True,\n",
    ").remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b017bc-35d3-478b-b126-c9977c0b182b",
   "metadata": {},
   "source": [
    "Creamos el trainer y entrenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1979bad2-e183-47e9-a779-068a8213a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2660\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2394' max='2660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2394/2660 04:12 < 00:28, 9.47 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.268600</td>\n",
       "      <td>1.052580</td>\n",
       "      <td>0.565484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.948625</td>\n",
       "      <td>0.634166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>1.026981</td>\n",
       "      <td>0.660112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>1.291819</td>\n",
       "      <td>0.654723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>1.571378</td>\n",
       "      <td>0.664581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>1.540023</td>\n",
       "      <td>0.681019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>1.776358</td>\n",
       "      <td>0.677361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>1.845957</td>\n",
       "      <td>0.681624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>2.014678</td>\n",
       "      <td>0.676105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-266\n",
      "Configuration saved in distil_prueba/checkpoint-266/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-266/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-266/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-266/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-532\n",
      "Configuration saved in distil_prueba/checkpoint-532/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-532/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-532/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-532/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-798\n",
      "Configuration saved in distil_prueba/checkpoint-798/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-798/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-1064\n",
      "Configuration saved in distil_prueba/checkpoint-1064/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-1064/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-1064/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-1064/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-1330\n",
      "Configuration saved in distil_prueba/checkpoint-1330/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-1330/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-1330/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-1330/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-1596\n",
      "Configuration saved in distil_prueba/checkpoint-1596/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-1596/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-1862\n",
      "Configuration saved in distil_prueba/checkpoint-1862/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-1862/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-1862/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-1862/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-2128\n",
      "Configuration saved in distil_prueba/checkpoint-2128/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-2128/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-2128/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-2128/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distil_prueba/checkpoint-2394\n",
      "Configuration saved in distil_prueba/checkpoint-2394/config.json\n",
      "Model weights saved in distil_prueba/checkpoint-2394/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/checkpoint-2394/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/checkpoint-2394/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distil_prueba/checkpoint-2128 (score: 0.6816242677303823).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2394, training_loss=0.3878119989744105, metrics={'train_runtime': 252.7338, 'train_samples_per_second': 168.161, 'train_steps_per_second': 10.525, 'total_flos': 1011932750879220.0, 'train_loss': 0.3878119989744105, 'epoch': 9.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26156444-ad1a-4378-9c94-7e6a65a9935e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59264f3fc1049439d143a2f6f8143ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 3798\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative      0.780     0.598     0.677       592\n",
      "          Negative      0.600     0.679     0.637      1041\n",
      "           Neutral      0.709     0.674     0.691       619\n",
      "          Positive      0.557     0.668     0.608       947\n",
      "Extremely Positive      0.787     0.581     0.669       599\n",
      "\n",
      "          accuracy                          0.647      3798\n",
      "         macro avg      0.687     0.640     0.656      3798\n",
      "      weighted avg      0.665     0.647     0.650      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_f = datasets.Dataset.from_dict({\"text\": X_test, \"labels\": y_test})\n",
    "\n",
    "tokenized_test = test_f.map(\n",
    "    tokenize_and_create_labels,\n",
    "    batched=True,\n",
    ").remove_columns([\"text\"])\n",
    "\n",
    "preds = trainer.predict(tokenized_test)\n",
    "print(classification_report(list(y_test), np.argmax(preds.predictions, axis=1), target_names=list(label2id), digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c677b498-2188-4706-a092-4b0550b553e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "\u001b[32m[I 2022-04-28 17:32:16,180]\u001b[0m A new study created in memory with name: no-name-dbdcfb46-2056-42b4-8bf6-fc12d01af831\u001b[0m\n",
      "Trial:\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 531\n",
      "Saving model checkpoint to distil_prueba/run-0/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-0/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5966, 'learning_rate': 2.1481699204200208e-05, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-0/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 75.8467, 'train_samples_per_second': 168.102, 'train_steps_per_second': 7.001, 'train_loss': 1.5947526188220007, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:33:34,663]\u001b[0m Trial 0 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.00036796071862678424, 'per_device_train_batch_size': 8, 'weight_decay': 1.2130421786377446e-06, 'gradient_accumulation_steps': 3}. Best is trial 0 with value: 0.07982924226254001.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5910857915878296, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3599, 'eval_samples_per_second': 551.53, 'eval_steps_per_second': 69.125, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1593\n",
      "Saving model checkpoint to distil_prueba/run-1/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-1/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.67, 'learning_rate': 0.003690051303775658, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-1/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-1/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-1/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to distil_prueba/run-1/checkpoint-1000\n",
      "Configuration saved in distil_prueba/run-1/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5763, 'learning_rate': 0.002002013195918541, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-1/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-1/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-1/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to distil_prueba/run-1/checkpoint-1500\n",
      "Configuration saved in distil_prueba/run-1/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5804, 'learning_rate': 0.00031397508806142374, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-1/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-1/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-1/checkpoint-1500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 114.6534, 'train_samples_per_second': 111.205, 'train_steps_per_second': 13.894, 'train_loss': 1.6063657809710696, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:35:32,054]\u001b[0m Trial 1 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.005378089411632775, 'per_device_train_batch_size': 4, 'weight_decay': 2.696828352963448e-06, 'gradient_accumulation_steps': 2}. Best is trial 0 with value: 0.07982924226254001.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5909756422042847, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3895, 'eval_samples_per_second': 539.746, 'eval_steps_per_second': 67.648, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 798\n",
      "Saving model checkpoint to distil_prueba/run-2/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-2/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5997, 'learning_rate': 0.00010620800785466605, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-2/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 75.7718, 'train_samples_per_second': 168.268, 'train_steps_per_second': 10.532, 'train_loss': 1.5926020342604559, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:36:50,335]\u001b[0m Trial 2 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.00028440936331551516, 'per_device_train_batch_size': 16, 'weight_decay': 2.5052861259104426e-05, 'gradient_accumulation_steps': 1}. Best is trial 0 with value: 0.07982924226254001.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.590785264968872, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3626, 'eval_samples_per_second': 550.42, 'eval_steps_per_second': 68.986, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1593\n",
      "Saving model checkpoint to distil_prueba/run-3/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-3/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6184, 'learning_rate': 0.0005963159325331751, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-3/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-3/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-3/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to distil_prueba/run-3/checkpoint-1000\n",
      "Configuration saved in distil_prueba/run-3/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5833, 'learning_rate': 0.0003235273083185479, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-3/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-3/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-3/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to distil_prueba/run-3/checkpoint-1500\n",
      "Configuration saved in distil_prueba/run-3/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5831, 'learning_rate': 5.0738684103920655e-05, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-3/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-3/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-3/checkpoint-1500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 115.5909, 'train_samples_per_second': 110.303, 'train_steps_per_second': 13.781, 'train_loss': 1.5930328369140625, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:38:48,550]\u001b[0m Trial 3 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.0008691045567478023, 'per_device_train_batch_size': 4, 'weight_decay': 0.00911292051158684, 'gradient_accumulation_steps': 2}. Best is trial 0 with value: 0.07982924226254001.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.591450572013855, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.388, 'eval_samples_per_second': 540.347, 'eval_steps_per_second': 67.723, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 798\n",
      "Saving model checkpoint to distil_prueba/run-4/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-4/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5981, 'learning_rate': 0.00032718313755448504, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-4/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 80.957, 'train_samples_per_second': 157.491, 'train_steps_per_second': 9.857, 'train_loss': 1.5914098218568884, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:40:12,033]\u001b[0m Trial 4 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.000876148133451272, 'per_device_train_batch_size': 8, 'weight_decay': 0.0033240874724519863, 'gradient_accumulation_steps': 2}. Best is trial 0 with value: 0.07982924226254001.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5917277336120605, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3563, 'eval_samples_per_second': 552.956, 'eval_steps_per_second': 69.304, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 264\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 65.9639, 'train_samples_per_second': 193.287, 'train_steps_per_second': 4.002, 'train_loss': 1.6081233169093276, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:41:20,623]\u001b[0m Trial 5 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.0010831682697942382, 'per_device_train_batch_size': 16, 'weight_decay': 1.9139840516727824e-12, 'gradient_accumulation_steps': 3}. Best is trial 0 with value: 0.07982924226254001.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5913418531417847, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3811, 'eval_samples_per_second': 543.04, 'eval_steps_per_second': 68.061, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 531\n",
      "Saving model checkpoint to distil_prueba/run-6/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-6/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6307, 'learning_rate': 0.0003147748411202089, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-6/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 76.5083, 'train_samples_per_second': 166.649, 'train_steps_per_second': 6.94, 'train_loss': 1.6269346262326574, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:42:39,691]\u001b[0m Trial 6 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.005391788407575191, 'per_device_train_batch_size': 8, 'weight_decay': 4.462278192301384e-07, 'gradient_accumulation_steps': 3}. Best is trial 0 with value: 0.07982924226254001.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.590257167816162, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3643, 'eval_samples_per_second': 549.723, 'eval_steps_per_second': 68.899, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 1062\n",
      "Saving model checkpoint to distil_prueba/run-7/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-7/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5928, 'learning_rate': 0.00012690279631802159, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-7/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-7/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-7/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to distil_prueba/run-7/checkpoint-1000\n",
      "Configuration saved in distil_prueba/run-7/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5716, 'learning_rate': 1.3999952618714126e-05, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-7/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-7/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-7/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 104.9921, 'train_samples_per_second': 121.438, 'train_steps_per_second': 10.115, 'train_loss': 1.577808035295562, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:44:27,227]\u001b[0m Trial 7 finished with value: 0.1432597239249152 and parameters: {'learning_rate': 0.00023980564001732906, 'per_device_train_batch_size': 4, 'weight_decay': 3.6985111737686066e-06, 'gradient_accumulation_steps': 3}. Best is trial 7 with value: 0.1432597239249152.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5501899719238281, 'eval_f1': 0.1432597239249152, 'eval_runtime': 1.3953, 'eval_samples_per_second': 537.535, 'eval_steps_per_second': 67.371, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 399\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 67.8083, 'train_samples_per_second': 188.03, 'train_steps_per_second': 5.884, 'train_loss': 1.6355212983630953, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:45:37,691]\u001b[0m Trial 8 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.004300592930654281, 'per_device_train_batch_size': 16, 'weight_decay': 8.66154084540421e-12, 'gradient_accumulation_steps': 2}. Best is trial 7 with value: 0.1432597239249152.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5906915664672852, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3713, 'eval_samples_per_second': 546.924, 'eval_steps_per_second': 68.548, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/guillem.garcia/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1593\n",
      "Saving model checkpoint to distil_prueba/run-9/checkpoint-500\n",
      "Configuration saved in distil_prueba/run-9/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6671, 'learning_rate': 0.0016417060246900728, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-9/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-9/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-9/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to distil_prueba/run-9/checkpoint-1000\n",
      "Configuration saved in distil_prueba/run-9/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.583, 'learning_rate': 0.0008906968642646049, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-9/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-9/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-9/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to distil_prueba/run-9/checkpoint-1500\n",
      "Configuration saved in distil_prueba/run-9/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5828, 'learning_rate': 0.00013968770383913703, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in distil_prueba/run-9/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distil_prueba/run-9/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distil_prueba/run-9/checkpoint-1500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 750\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 114.9728, 'train_samples_per_second': 110.896, 'train_steps_per_second': 13.855, 'train_loss': 1.6082832446580253, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 17:47:35,301]\u001b[0m Trial 9 finished with value: 0.07982924226254001 and parameters: {'learning_rate': 0.0023927151851155406, 'per_device_train_batch_size': 4, 'weight_decay': 2.0132350183619526e-10, 'gradient_accumulation_steps': 2}. Best is trial 7 with value: 0.1432597239249152.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5902090072631836, 'eval_f1': 0.07982924226254001, 'eval_runtime': 1.3893, 'eval_samples_per_second': 539.842, 'eval_steps_per_second': 67.66, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='7', objective=0.1432597239249152, hyperparameters={'learning_rate': 0.00023980564001732906, 'per_device_train_batch_size': 4, 'weight_decay': 3.6985111737686066e-06, 'gradient_accumulation_steps': 3})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"distil_prueba\",\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    eval_steps=500,\n",
    "    disable_tqdm=True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]), #32, 64\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-12, 1e-1, log=True),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2, 3]),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    backend=\"optuna\", \n",
    "    n_trials=10,\n",
    "    hp_space=hp_space,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf12b4-0ff3-4267-b421-17dc358e8792",
   "metadata": {},
   "source": [
    "## Posibles mejoras\n",
    "* Data augmentation\n",
    "* Usar modelos más grandes\n",
    "* Usar modelos entrenados con tweets solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605638c1-0b19-424d-bd8a-f469ebc1d893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
